  1) releases/2021b
job start at Sat Mar 23 10:07:46 CET 2024
2024-03-23 10:07:51.026441: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2024-03-23 10:07:51.027233: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2024-03-23 10:07:52.154327: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2024-03-23 10:07:52.154759: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2600000000 Hz
 
first y shape:  (112000, 8)
datagenerator definition successfull
start AI architecture definition
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input1 (InputLayer)             [(None, 2, 4096, 1)] 0                                            
__________________________________________________________________________________________________
input2 (InputLayer)             [(None, 2, 2048, 1)] 0                                            
__________________________________________________________________________________________________
input3 (InputLayer)             [(None, 2, 1024, 1)] 0                                            
__________________________________________________________________________________________________
input4 (InputLayer)             [(None, 2, 512, 1)]  0                                            
__________________________________________________________________________________________________
input5 (InputLayer)             [(None, 2, 256, 1)]  0                                            
__________________________________________________________________________________________________
input6 (InputLayer)             [(None, 2, 128, 1)]  0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 1, 4089, 40)  680         input1[0][0]                     
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 1, 2041, 40)  680         input2[0][0]                     
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 1, 1017, 40)  680         input3[0][0]                     
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 1, 505, 40)   680         input4[0][0]                     
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 1, 249, 40)   680         input5[0][0]                     
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 1, 121, 30)   510         input6[0][0]                     
__________________________________________________________________________________________________
dropout (Dropout)               (None, 1, 4089, 40)  0           conv2d[0][0]                     
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 1, 2041, 40)  0           conv2d_2[0][0]                   
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 1, 1017, 40)  0           conv2d_4[0][0]                   
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 1, 505, 40)   0           conv2d_6[0][0]                   
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 1, 249, 40)   0           conv2d_8[0][0]                   
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 1, 121, 30)   0           conv2d_10[0][0]                  
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 1, 4086, 10)  1610        dropout[0][0]                    
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 1, 2038, 10)  1610        dropout_2[0][0]                  
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 1, 1014, 10)  1610        dropout_4[0][0]                  
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 1, 502, 10)   1610        dropout_6[0][0]                  
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 1, 246, 40)   6440        dropout_8[0][0]                  
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 1, 118, 10)   1210        dropout_10[0][0]                 
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 1, 4086, 10)  0           conv2d_1[0][0]                   
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 1, 2038, 10)  0           conv2d_3[0][0]                   
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 1, 1014, 10)  0           conv2d_5[0][0]                   
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 1, 502, 10)   0           conv2d_7[0][0]                   
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 1, 246, 40)   0           conv2d_9[0][0]                   
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 1, 118, 10)   0           conv2d_11[0][0]                  
__________________________________________________________________________________________________
flatten (Flatten)               (None, 40860)        0           dropout_1[0][0]                  
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 20380)        0           dropout_3[0][0]                  
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 10140)        0           dropout_5[0][0]                  
__________________________________________________________________________________________________
flatten_3 (Flatten)             (None, 5020)         0           dropout_7[0][0]                  
__________________________________________________________________________________________________
flatten_4 (Flatten)             (None, 9840)         0           dropout_9[0][0]                  
__________________________________________________________________________________________________
flatten_5 (Flatten)             (None, 1180)         0           dropout_11[0][0]                 
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 87420)        0           flatten[0][0]                    
                                                                 flatten_1[0][0]                  
                                                                 flatten_2[0][0]                  
                                                                 flatten_3[0][0]                  
                                                                 flatten_4[0][0]                  
                                                                 flatten_5[0][0]                  
__________________________________________________________________________________________________
dense1 (Dense)                  (None, 256)          22379776    concatenate[0][0]                
__________________________________________________________________________________________________
dense2 (Dense)                  (None, 8)            2056        dense1[0][0]                     
__________________________________________________________________________________________________
reshape (Reshape)               (None, 8)            0           dense2[0][0]                     
==================================================================================================
Total params: 22,399,832
Trainable params: 22,399,832
Non-trainable params: 0
__________________________________________________________________________________________________
end of architecture
 
Training start
Epoch 1/100
2450/2450 - 1423s - loss: 1.6402 - accuracy: 0.3407 - val_loss: 1.1726 - val_accuracy: 0.4855
2024-03-23 10:31:36.755945: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.

Epoch 00001: val_loss improved from inf to 1.17259, saving model to ./weights_folder/fusion_4096_higher
Epoch 2/100
2450/2450 - 1414s - loss: 0.9977 - accuracy: 0.5603 - val_loss: 0.8963 - val_accuracy: 0.5974

Epoch 00002: val_loss improved from 1.17259 to 0.89631, saving model to ./weights_folder/fusion_4096_higher
Epoch 3/100
2450/2450 - 1411s - loss: 0.8188 - accuracy: 0.6483 - val_loss: 0.8106 - val_accuracy: 0.6549

Epoch 00003: val_loss improved from 0.89631 to 0.81060, saving model to ./weights_folder/fusion_4096_higher
Epoch 4/100
2450/2450 - 1409s - loss: 0.6756 - accuracy: 0.7196 - val_loss: 0.7419 - val_accuracy: 0.6939

Epoch 00004: val_loss improved from 0.81060 to 0.74194, saving model to ./weights_folder/fusion_4096_higher
Epoch 5/100
2450/2450 - 1404s - loss: 0.5392 - accuracy: 0.7861 - val_loss: 0.5918 - val_accuracy: 0.7632

Epoch 00005: val_loss improved from 0.74194 to 0.59176, saving model to ./weights_folder/fusion_4096_higher
Epoch 6/100
2450/2450 - 1404s - loss: 0.4197 - accuracy: 0.8405 - val_loss: 0.5271 - val_accuracy: 0.7886

Epoch 00006: val_loss improved from 0.59176 to 0.52706, saving model to ./weights_folder/fusion_4096_higher
Epoch 7/100
2450/2450 - 1400s - loss: 0.3411 - accuracy: 0.8741 - val_loss: 0.4814 - val_accuracy: 0.8174

Epoch 00007: val_loss improved from 0.52706 to 0.48140, saving model to ./weights_folder/fusion_4096_higher
Epoch 8/100
2450/2450 - 1400s - loss: 0.2740 - accuracy: 0.9009 - val_loss: 0.4178 - val_accuracy: 0.8444

Epoch 00008: val_loss improved from 0.48140 to 0.41784, saving model to ./weights_folder/fusion_4096_higher
Epoch 9/100
2450/2450 - 1394s - loss: 0.2370 - accuracy: 0.9151 - val_loss: 0.3756 - val_accuracy: 0.8579

Epoch 00009: val_loss improved from 0.41784 to 0.37559, saving model to ./weights_folder/fusion_4096_higher
Epoch 10/100
2450/2450 - 1398s - loss: 0.2022 - accuracy: 0.9287 - val_loss: 0.3636 - val_accuracy: 0.8632

Epoch 00010: val_loss improved from 0.37559 to 0.36361, saving model to ./weights_folder/fusion_4096_higher
Epoch 11/100
2450/2450 - 1395s - loss: 0.1794 - accuracy: 0.9370 - val_loss: 0.3357 - val_accuracy: 0.8772

Epoch 00011: val_loss improved from 0.36361 to 0.33568, saving model to ./weights_folder/fusion_4096_higher
Epoch 12/100
2450/2450 - 1331s - loss: 0.1666 - accuracy: 0.9437 - val_loss: 0.3284 - val_accuracy: 0.8782

Epoch 00012: val_loss improved from 0.33568 to 0.32845, saving model to ./weights_folder/fusion_4096_higher
Epoch 13/100
2450/2450 - 1311s - loss: 0.1434 - accuracy: 0.9508 - val_loss: 0.3166 - val_accuracy: 0.8875

Epoch 00013: val_loss improved from 0.32845 to 0.31658, saving model to ./weights_folder/fusion_4096_higher
Epoch 14/100
2450/2450 - 1313s - loss: 0.1370 - accuracy: 0.9541 - val_loss: 0.3074 - val_accuracy: 0.8901

Epoch 00014: val_loss improved from 0.31658 to 0.30741, saving model to ./weights_folder/fusion_4096_higher
Epoch 15/100
2450/2450 - 1311s - loss: 0.1285 - accuracy: 0.9566 - val_loss: 0.2808 - val_accuracy: 0.9050

Epoch 00015: val_loss improved from 0.30741 to 0.28079, saving model to ./weights_folder/fusion_4096_higher
Epoch 16/100
2450/2450 - 1313s - loss: 0.1169 - accuracy: 0.9618 - val_loss: 0.2769 - val_accuracy: 0.9042

Epoch 00016: val_loss improved from 0.28079 to 0.27691, saving model to ./weights_folder/fusion_4096_higher
Epoch 17/100
2450/2450 - 1313s - loss: 0.1129 - accuracy: 0.9640 - val_loss: 0.3231 - val_accuracy: 0.8860

Epoch 00017: val_loss did not improve from 0.27691
Epoch 18/100
2450/2450 - 1308s - loss: 0.1041 - accuracy: 0.9663 - val_loss: 0.2834 - val_accuracy: 0.9024

Epoch 00018: val_loss did not improve from 0.27691
Epoch 19/100
2450/2450 - 1308s - loss: 0.0983 - accuracy: 0.9689 - val_loss: 0.2883 - val_accuracy: 0.8982

Epoch 00019: val_loss did not improve from 0.27691
Epoch 20/100
2450/2450 - 1313s - loss: 0.1036 - accuracy: 0.9679 - val_loss: 0.2673 - val_accuracy: 0.9095

Epoch 00020: val_loss improved from 0.27691 to 0.26729, saving model to ./weights_folder/fusion_4096_higher
Epoch 21/100
2450/2450 - 1310s - loss: 0.0982 - accuracy: 0.9686 - val_loss: 0.2792 - val_accuracy: 0.9032

Epoch 00021: val_loss did not improve from 0.26729
Epoch 22/100
2450/2450 - 1337s - loss: 0.0857 - accuracy: 0.9724 - val_loss: 0.2858 - val_accuracy: 0.9039

Epoch 00022: val_loss did not improve from 0.26729
Epoch 23/100
2450/2450 - 1393s - loss: 0.0883 - accuracy: 0.9726 - val_loss: 0.2680 - val_accuracy: 0.9115

Epoch 00023: val_loss did not improve from 0.26729
Epoch 24/100
2450/2450 - 1377s - loss: 0.0842 - accuracy: 0.9748 - val_loss: 0.2391 - val_accuracy: 0.9227

Epoch 00024: val_loss improved from 0.26729 to 0.23914, saving model to ./weights_folder/fusion_4096_higher
Epoch 25/100
2450/2450 - 1380s - loss: 0.0783 - accuracy: 0.9757 - val_loss: 0.2632 - val_accuracy: 0.9103

Epoch 00025: val_loss did not improve from 0.23914
Epoch 26/100
2450/2450 - 1374s - loss: 0.0807 - accuracy: 0.9750 - val_loss: 0.2949 - val_accuracy: 0.9046

Epoch 00026: val_loss did not improve from 0.23914
Epoch 27/100
2450/2450 - 1378s - loss: 0.0791 - accuracy: 0.9763 - val_loss: 0.2504 - val_accuracy: 0.9191

Epoch 00027: val_loss did not improve from 0.23914
Epoch 28/100
2450/2450 - 1384s - loss: 0.0727 - accuracy: 0.9770 - val_loss: 0.2566 - val_accuracy: 0.9217

Epoch 00028: val_loss did not improve from 0.23914
Epoch 29/100
2450/2450 - 1376s - loss: 0.0665 - accuracy: 0.9797 - val_loss: 0.2460 - val_accuracy: 0.9161

Epoch 00029: val_loss did not improve from 0.23914
Epoch 30/100
2450/2450 - 1382s - loss: 0.0729 - accuracy: 0.9782 - val_loss: 0.3192 - val_accuracy: 0.8972

Epoch 00030: val_loss did not improve from 0.23914
Epoch 31/100
2450/2450 - 1377s - loss: 0.0734 - accuracy: 0.9778 - val_loss: 0.2484 - val_accuracy: 0.9279

Epoch 00031: val_loss did not improve from 0.23914
Epoch 32/100
2450/2450 - 1376s - loss: 0.0686 - accuracy: 0.9799 - val_loss: 0.2862 - val_accuracy: 0.9162

Epoch 00032: val_loss did not improve from 0.23914
Epoch 33/100
2450/2450 - 1378s - loss: 0.0680 - accuracy: 0.9795 - val_loss: 0.2715 - val_accuracy: 0.9178

Epoch 00033: val_loss did not improve from 0.23914
Epoch 34/100
2450/2450 - 1379s - loss: 0.0768 - accuracy: 0.9781 - val_loss: 0.2637 - val_accuracy: 0.9217

Epoch 00034: val_loss did not improve from 0.23914
Epoch 00034: early stopping
training time:  46580.21468234062 seconds 
end of training
 
Loading model
end of loading
 
Evaluating model
Traceback (most recent call last):
  File "dyadic_s_fusion.py", line 402, in <module>
    score = model.evaluate(DataGenerator(test_idx, y, 32, vec_len, shuffle=True), verbose=1)
  File "/opt/cecisw/arch/easybuild/2020b/software/TensorFlow/2.4.1-foss-2020b/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 1389, in evaluate
    tmp_logs = self.test_function(iterator)
  File "/opt/cecisw/arch/easybuild/2020b/software/TensorFlow/2.4.1-foss-2020b/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 828, in __call__
    result = self._call(*args, **kwds)
  File "/opt/cecisw/arch/easybuild/2020b/software/TensorFlow/2.4.1-foss-2020b/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 894, in _call
    return self._concrete_stateful_fn._call_flat(
  File "/opt/cecisw/arch/easybuild/2020b/software/TensorFlow/2.4.1-foss-2020b/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 1918, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File "/opt/cecisw/arch/easybuild/2020b/software/TensorFlow/2.4.1-foss-2020b/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 555, in call
    outputs = execute.execute(
  File "/opt/cecisw/arch/easybuild/2020b/software/TensorFlow/2.4.1-foss-2020b/lib/python3.8/site-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError:  Input to reshape is a tensor with 324480 values, but the requested shape requires a multiple of 1180
	 [[node model/flatten_2/Reshape (defined at dyadic_s_fusion.py:402) ]] [Op:__inference_test_function_312995]

Function call stack:
test_function

2024-03-23 23:04:17.731290: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
job end at Sat Mar 23 23:04:18 CET 2024
